<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving">
  <meta name="keywords" content="VLA, Autonomous driving, VLM, LLM, MLLM, CoT">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-LYK5B1RWXZ"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-LYK5B1RWXZ');
</script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>


<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving</h1>

          <div class="is-size-4 publication-authors">
            <span class="author-block">

          </div>

          <div class="is-size-4 publication-authors">
            <span class="author-block">
              Shuang Zeng<sup>1, 2 *</sup>,</span>
            <span class="author-block">
              Xinyuan Chang<sup>1 </sup>,</span>
            <span class="author-block">
              Mengwei Xie<sup>1 </sup>,</span>
            <span class="author-block">
              Xinran Liu<sup>1</sup>,</span>
            <span class="author-block">
              Yifan Bai<sup>3 </sup>,</span>      
            <span class="author-block">
              Zheng Pan<sup>1</sup>,</span>
            <span class="author-block">
              Mu Xu<sup>1 </sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=KNyC5EUAAAAJ&hl=zh-CN&oi=ao/">Xing Wei</a><sup>2†</sup>
            </span>
          </div>

          <div class="is-size-5 publication-affiliations">
            <span class="author-block"> <sup>1</sup>  Amap, Alibaba Group </span>
            &nbsp &nbsp
            <span class="author-block"> <sup>2</sup> Xi’an Jiaotong University </span>
            &nbsp &nbsp
            <span class="author-block"> <sup>3</sup> DAMO Academy, Alibaba Group </span>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2409.05352"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2409.05352"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/missTL/PriorDrive"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span >Code </span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
            <div class="hero-body" style="text-align: center">
                <div class="columns is-centered">
                     <img src="./static/images/compare.png" alt="Description of the image" style="max-width: 100%; height: auto;" />
                </div>
            </div>
  </div>
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Comparison of different CoT. Text CoT expression provides insufficient information. 
            The modalities between the image-text CoT are inconsistent. The proposed spatio-temporal 
            CoT captures the temporal and spatial relationships in the future.
          </p>
        </div>
      </div>
    </div>
</section>  


<section class="section">
    <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           Visual language models (VLMs) have attracted increasing interest in autonomous driving due to their powerful 
            reasoning capabilities. However, existing VLMs typically utilize discrete text Chain-of-Thought (CoT) tailored 
            to the current scenario, which essentially represents highly abstract and symbolic compression of visual information, 
            potentially leading to spatio-temporal relationship ambiguity and fine-grained information loss. Is autonomous driving 
            better modeled on real-world simulation and imagination than on pure symbolic logic? 
          </p>
          <p>
           In this paper, we propose a spatio-temporal CoT reasoning method that enables models to think visually. First, VLM serves 
            as a world model to generate unified image frame for predicting future world states: where perception results (e.g., lane 
            divider and 3D detection) represent the future spatial relationships, and ordinary future frame represent the temporal evolution 
            relationships. This spatio-temporal CoT then serves as intermediate reasoning steps, enabling the VLM to function as an inverse 
            dynamics model for trajectory planning based on current observations and future predictions. To implement visual generation in VLMs,
            we propose a unified pretraining paradigm integrating visual generation and understanding, along with a progressive visual CoT enhancing 
            autoregressive image generation. Extensive experimental results demonstrate the effectiveness of the proposed method, advancing autonomous 
            driving towards visual reasoning.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2">Method</h2>
            <div class="hero-body" style="text-align: center">
                <div class="columns is-centered">
                     <img src="./static/images/method_final.png" alt="Description of the image" style="max-width: 100%; height: auto;" />
                </div>
            </div>
  </div>
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Overview of FSDrive. Taking the currently surround images and task instructions as input, 
            MLLM is trained in the form of next token prediction. MLLM predicts the future spatio-temporal
            CoT, and then generates trajectory based on the current observation and predicted future.
          </p>
        </div>
      </div>
    </div>
</section> 


<section class="section">
  <div class="container is-max-desktop">
            <div class="columns">
               <h2 class="title is-2">Video</h2>
            </div>
            <div class="hero-body" style="text-align: center">
                <div class="columns is-centered">
                  <video poster="" autoplay="" muted="" loop="" style="pointer-events: none;">
                    <source src="./static/videos/video1.mp4" type="video/mp4">
                  </video>
                </div>
            </div>
            <div class="hero-body" style="text-align: center">
                <div class="columns is-centered">
                  <video poster="" autoplay="" muted="" loop="" style="pointer-events: none;">
                    <source src="./static/videos/video2.mp4" type="video/mp4">
                  </video>
                </div>
            </div>
            <div class="hero-body" style="text-align: center">
                <div class="columns is-centered">
                  <video poster="" autoplay="" muted="" loop="" style="pointer-events: none;">
                    <source src="./static/videos/video3.mp4" type="video/mp4">
                  </video>
                </div>
            </div>
            <div class="hero-body" style="text-align: center">
                <div class="columns is-centered">
                  <video poster="" autoplay="" muted="" loop="" style="pointer-events: none;">
                    <source src="./static/videos/video4.mp4" type="video/mp4">
                  </video>
                </div>
            </div>
  </div>
</section>  


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-2">Qualitative results</h2>
            <div class="hero-body" style="text-align: center">
                <div class="columns is-centered">
                     <img src="./static/images/vis.png" alt="Description of the image" style="max-width: 100%; height: auto;" />
                </div>
            </div>
  </div>
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            Qualitative analysis of our CoT. The red trajectory is the prediction and the green is the GT.
          </p>
        </div>
      </div>
    </div>
</section>  
  

<section class="section">
  <div class="container is-max-desktop">
        <h2 class="title is-2">Main results</h2>
            <div class="hero-body" style="text-align: center">
                <div class="columns is-centered">
                     <img src="./static/images/main.jpg" alt="Description of the image" style="max-width: 100%; height: auto;" />
                </div>
            </div>
  </div>
  <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <p>
            End-to-end trajectory planning experiments on nuScenes. We evaluated the L2 and collision metrics 
            based on the distinct computational methodologies of ST-P3 and UniAD, respectively. * indicates that
            the ego status is additionally used. VAD and UniAD results are derived from BEV-Planner, while the 
            remaining results are sourced from their respective papers.
          </p>
        </div>
      </div>
    </div>
</section>  
  
  


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p>
      If you find our work useful in your research, please cite our paper:
    </p>  
    <pre><code>@article{zeng2024driving,
  title={Driving with Prior Maps: Unified Vector Prior Encoding for Autonomous Vehicle Mapping},
  author={Zeng, Shuang and Chang, Xinyuan and Liu, Xinran and Pan, Zheng and Wei, Xing},
  journal={arXiv preprint arXiv:2409.05352},
  year={2024}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align: center">
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
